{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Overview\n",
    "Nema Sobhani / ML Team  \n",
    "2020.12.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Drug Dosage Efficacy\n",
    "\n",
    "*Working through example from [StatQuest](https://www.youtube.com/watch?v=CqOfi41LfDw) NN video series*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "***We want to predict whether a drug's dosage will be effective, given the following data:***\n",
    "\n",
    "<img src=\"images/capture.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "There is a dinstinct **non-linearity** to the curve our model should produce. So we will want a model that can inject bits of non-linearity, that sum to create this final curve.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms\n",
    "\n",
    "1. **Input Neurons**\n",
    "    - Simply the input values\n",
    "    - This may be as simple as a single scalar (this example), all the way up to a high-rank tensor\n",
    "    \n",
    "    \n",
    "2. **Hidden Layer**\n",
    "    - Next layer of neurons\n",
    "    - These are fully connected to the previous and next layer\n",
    "    - Characterized by an **activation function** which will generate some portion of our final, non-linear curve\n",
    "    \n",
    "    \n",
    "3. **Connections between neurons/nodes (_Weights and Biases_)**\n",
    "    - **weights** : which will stretch/compress and invert data from the input space\n",
    "    - **biases** : shift/offset the input data (think intercept)\n",
    "   \n",
    "   \n",
    "4. **Output Neurons**\n",
    "    - Map to our output classes\n",
    "    - May use an activation function that that varies from the intermediate activation functions\n",
    "        - In order to map output values to probabilities between [0-1]\n",
    "    - Where **loss function** is calculated, kicking off **back propogation**\n",
    "\n",
    "\n",
    "<img src=\"images/capture02.png\" width=\"750\">\n",
    "\n",
    "Assume fully trained model:\n",
    "<img src=\"images/capture01.png\" width=\"750\">\n",
    "\n",
    "<img src=\"images/capture03.png\" width=\"750\">\n",
    "\n",
    "<img src=\"images/capture04.png\" width=\"750\">\n",
    "\n",
    "***How do you know what your weights and biases should be?*** ðŸ‘‰ **BACK PROPOGATION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUICK NOTES:**\n",
    "\n",
    "- Weights are typically randomly selected from a standard normal, and biases are initialized to 0\n",
    "\n",
    "\n",
    "- The optimal number of neurons per hidden layer can follow a rule of thumb or be derived from experimentation\n",
    "\n",
    "\n",
    "- Each intermediate neuron's activation function *paired* with the stretching/compressing/shifting from the weights and biases of subsequent connections creates infinite possibilities for creating non-linear fragments which comprise your final model in its dimension-space. ([DESMOS](https://www.desmos.com/calculator/cnmsavileq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High Level:**\n",
    "\n",
    "- Once we reach the end of our network, we calculate error with a **loss function** and get the resulting error, given a parameter's value. \n",
    "    - *But what are our parameters and how many do we have?*\n",
    "\n",
    "\n",
    "- Every **weight** and **bias** is a parameter! In the example above, we have 7 parameters. Think of this as modeling error in 7-dimensions!\n",
    "\n",
    "\n",
    "- Tools at our disposal:\n",
    "    - **Loss functions** (wow cool)\n",
    "    - **Chain rule of Calculus** to calculate derivatives with respect to each component's value and error\n",
    "    - **Gradient Descent** which will find a local minimum of error by calculating error derivatives against the paramater value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Walkthrough**:\n",
    "\n",
    "<img src=\"images/capture05.png\" width=\"750\">\n",
    "\n",
    "1. After feed-forward, we now calculate our error wrt the label\n",
    "\n",
    "\n",
    "2. Our error is likely to be high, due to random parameter assignment\n",
    "\n",
    "\n",
    "3. To reduce error, we go backwards and try to minimize error one parameter at a time\n",
    "\n",
    "\n",
    "- Our last bias is *composed* of many other parameters, meaning that it is a *function* of them.\n",
    "- In order to calculate the gradient of the derivative of the error wrt this parameter, we must write out a ridiculous chain rule composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We recursively traverse backward towards the start and unravel once the partial derivatives are calculated. Let's build up to where we are:  \n",
    "$f =$ activation function\n",
    "\n",
    "#### Top Node\n",
    "\n",
    "$W_1 x$\n",
    "\n",
    "$W_1 x + b_1$\n",
    "\n",
    "$f(W_1 x + b_1)$\n",
    "\n",
    "$W_3 (f(W_1 x + b_1))$\n",
    "\n",
    "___\n",
    "\n",
    "#### Bottom Node\n",
    "\n",
    "$W_2 x$\n",
    "\n",
    "$W_2 x + b_2$\n",
    "\n",
    "$f(W_2 x + b_2)$\n",
    "\n",
    "$W_4 (f(W_2 x + b_2))$\n",
    "\n",
    "___\n",
    "\n",
    "#### Full Composition\n",
    "\n",
    "$(W_3 (f(W_1 x + b_1) + W_4 (f(W_2 x + b_2))) + \\boldsymbol{b_3} \\ \\ $ ðŸ‘ˆ you are here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use gradient descent to determine where the **direction of steepest drop is for the error with regard to each parameter**, it is dependent on knowing all of the derivatives with respect to the downstream parameters.\n",
    "\n",
    "$$\\frac{d ERR}{d \\boldsymbol{b_3}} = \\frac{d ERR}{d \\text{Predicted}} \\cdot \\frac{d \\text{Predicted}}{d\\boldsymbol{b_3}}$$\n",
    "\n",
    "$$\\frac{d ERR}{d \\boldsymbol{W_1}} = \\frac{d ERR}{d \\text{Predicted}} \\cdot \\frac{d \\text{Predicted}}{d (f(W_1 x + b_1))} \\cdot \\frac{d (f(W_1 x + b_1))}{d (W_1 x + b_1)} \\cdot \\frac{d (W_1 x + b_1)}{d \\boldsymbol{W_1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long story short:**\n",
    "\n",
    "Every parameter will adjust, proportional to their effect on reducing error, and the process will repeat until stable (ideally, prior to overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "- Comfortability with gradient descent (learning rate / step size / etc)\n",
    "- Chain rule of calculus (understanding partial derivatives in contribution to error minimization)\n",
    "- Extrapolate the math and concepts to vector inputs with matrix operations\n",
    "- Tensors - https://www.tensorflow.org/guide/tensor\n",
    "- Batch vs Epoch - https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
